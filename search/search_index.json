{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GoFAISS","text":"<p>A pure Go implementation of Facebook's FAISS (Facebook AI Similarity Search) library for efficient similarity search and clustering of dense vectors.</p>"},{"location":"#overview","title":"Overview","text":"<p>GoFAISS provides high-performance vector similarity search with multiple indexing strategies, distance metrics, and compression techniques. Built entirely in Go with zero external dependencies (except for benchmarking comparisons), it's designed for production use cases requiring fast nearest neighbor search.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#multiple-index-types","title":"Multiple Index Types","text":"<ul> <li>FlatIndex: Exact brute-force search with 100% recall</li> <li>HNSWIndex: Hierarchical Navigable Small World graphs for fastest approximate search</li> <li>IVFIndex: Inverted File Index with k-means clustering</li> <li>PQIndex: Product Quantization for memory-efficient storage</li> <li>IVFPQIndex: Combined IVF + PQ for balanced performance</li> </ul>"},{"location":"#distance-metrics","title":"Distance Metrics","text":"<ul> <li>L2 (Euclidean): Standard distance metric for embeddings</li> <li>Cosine Similarity: Normalized vector comparison</li> <li>Inner Product: Dot product similarity</li> </ul>"},{"location":"#production-features","title":"Production Features","text":"<ul> <li>Persistence: Save/load indexes with gzip compression</li> <li>Serialization: Both Gob and JSON format support</li> <li>Concurrency: Thread-safe operations</li> <li>Memory Efficiency: Product quantization achieves 32x compression</li> <li>Benchmarking: Built-in ANN benchmarking framework</li> </ul>"},{"location":"#performance-highlights","title":"Performance Highlights","text":"<p>Based on 100K vectors with 128 dimensions:</p> Index Type QPS Recall@10 Memory Build Time HNSW 24,087 98%+ 97.7 MB 18s IVFPQ 516 95% 1.0 MB 8.5s IVF 343 95% 49.0 MB 4.6s PQ 50 92% 1.7 MB 4.0s Flat 36 100% 48.8 MB 0.002s <p>Key Takeaways: - HNSW provides the fastest queries (24K QPS) with excellent recall - IVFPQ offers the best memory/performance tradeoff (98% compression) - All indexes are production-ready with consistent performance</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>package main\n\nimport (\n    \"github.com/tahcohcat/gofaiss/pkg/index/hnsw\"\n    \"github.com/tahcohcat/gofaiss/pkg/vector\"\n)\n\nfunc main() {\n    // Create HNSW index for 128-dimensional vectors\n    idx, _ := hnsw.New(128, \"l2\", hnsw.Config{\n        M:              16,\n        EfConstruction: 200,\n        EfSearch:       50,\n    })\n\n    // Add vectors\n    vectors := []vector.Vector{\n        {ID: 1, Data: make([]float32, 128)},\n        {ID: 2, Data: make([]float32, 128)},\n        // ... more vectors\n    }\n    idx.Add(vectors)\n\n    // Search for k nearest neighbors\n    query := make([]float32, 128)\n    results, _ := idx.Search(query, 10)\n\n    for _, result := range results {\n        println(\"ID:\", result.ID, \"Distance:\", result.Distance)\n    }\n\n    // Persist index\n    idx.SaveToFile(\"vectors.faiss.gz\")\n}\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>go get github.com/tahcohcat/gofaiss\n</code></pre> <p>Requirements: Go 1.21 or later</p>"},{"location":"#use-cases","title":"Use Cases","text":"<p>GoFAISS is ideal for:</p> <ul> <li>Semantic Search: Find similar documents, images, or other embeddings</li> <li>Recommendation Systems: Product recommendations based on user embeddings</li> <li>Deduplication: Identify near-duplicate content</li> <li>Clustering: Group similar vectors together</li> <li>Anomaly Detection: Find outliers in high-dimensional spaces</li> <li>RAG Systems: Retrieval-augmented generation for LLM applications</li> </ul>"},{"location":"#comparison-with-alternatives","title":"Comparison with Alternatives","text":""},{"location":"#vs-python-faiss","title":"vs Python FAISS","text":"<ul> <li>Pros: No Python dependency, native Go integration, smaller binary size</li> <li>Cons: Fewer index types (no GPU support yet)</li> <li>Use When: Building Go services, deploying to environments without Python</li> </ul>"},{"location":"#vs-hnswlib-go","title":"vs hnswlib-go","text":"<ul> <li>Pros: More index types, quantization support, better documentation</li> <li>Performance: 20-25% faster queries in benchmarks</li> <li>Use When: Need memory efficiency or multiple index strategies</li> </ul>"},{"location":"#vs-pure-vector-databases","title":"vs Pure Vector Databases","text":"<ul> <li>Pros: Embeddable library, no separate service, simpler deployment</li> <li>Cons: No distributed search, no query language</li> <li>Use When: Single-node applications, embedded systems, simplicity matters</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>GoFAISS follows a modular architecture:</p> <pre><code>gofaiss/\n\u251c\u2500\u2500 pkg/\n\u2502   \u251c\u2500\u2500 metric/       # Distance calculations (L2, cosine, IP)\n\u2502   \u251c\u2500\u2500 index/        # Index implementations\n\u2502   \u2502   \u251c\u2500\u2500 flat/     # Brute force search\n\u2502   \u2502   \u251c\u2500\u2500 hnsw/     # Graph-based ANN\n\u2502   \u2502   \u251c\u2500\u2500 ivf/      # Inverted file index\n\u2502   \u2502   \u251c\u2500\u2500 pq/       # Product quantization\n\u2502   \u2502   \u2514\u2500\u2500 ivfpq/    # Combined IVF+PQ\n\u2502   \u251c\u2500\u2500 vector/       # Vector utilities\n\u2502   \u251c\u2500\u2500 storage/      # Persistence layer\n\u2502   \u2514\u2500\u2500 search/       # High-level search API\n\u251c\u2500\u2500 internal/\n\u2502   \u251c\u2500\u2500 math/         # Optimized math operations\n\u2502   \u2514\u2500\u2500 utils/        # Helper functions\n\u2514\u2500\u2500 cmd/\n    \u251c\u2500\u2500 gofaiss-cli/     # Command-line interface\n    \u2514\u2500\u2500 gofaiss-server/  # HTTP server\n</code></pre>"},{"location":"#project-status","title":"Project Status","text":"<p>Current Version: 0.1.0 (Alpha)</p> <p>GoFAISS is under active development. The core functionality is stable and production-ready, but the API may change before v1.0.</p> <p>Production Readiness: - Core search algorithms -  Persistence and serialization - Comprehensive benchmarks - Thread safety</p> <p>TODO - API stability (pre-v1.0) - Distributed search - GPU acceleration</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please see our GitHub repository for:</p> <ul> <li>Bug reports and feature requests</li> <li>Pull requests</li> <li>Documentation improvements</li> <li>Benchmark results from your environment</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE file for details</p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<ul> <li>Inspired by Facebook's FAISS</li> <li>HNSW algorithm from Malkov &amp; Yashunin (2016)</li> <li>Product Quantization from J\u00e9gou et al. (2011)</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started Guide - Detailed installation and usage</li> <li>Architecture Overview - Deep dive into implementation</li> <li>Benchmarks - Comprehensive performance analysis</li> <li>API Documentation - Full API reference</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Complete API documentation for GoFAISS.</p>"},{"location":"api/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Vector Package</li> <li>Metric Package</li> <li>Index Types</li> <li>Flat Index</li> <li>HNSW Index</li> <li>IVF Index</li> <li>PQ Index</li> <li>IVFPQ Index</li> <li>Storage Package</li> <li>Stats Package</li> </ul>"},{"location":"api/#vector-package","title":"Vector Package","text":"<p><code>import \"github.com/tahcohcat/gofaiss/pkg/vector\"</code></p>"},{"location":"api/#types","title":"Types","text":""},{"location":"api/#vector","title":"Vector","text":"<pre><code>type Vector struct {\n    ID   int64     // Vector identifier\n    Data []float32 // Vector data\n    Norm float32   // Precomputed norm for cosine similarity\n}\n</code></pre> <p>Represents a vector with associated ID and optional precomputed norm.</p>"},{"location":"api/#searchresult","title":"SearchResult","text":"<pre><code>type SearchResult struct {\n    ID       int64   // Vector ID\n    Distance float32 // Distance to query\n}\n</code></pre> <p>Represents a search result with distance metric.</p>"},{"location":"api/#functions","title":"Functions","text":""},{"location":"api/#generaterandom","title":"GenerateRandom","text":"<pre><code>func GenerateRandom(n, dim int, seed int64) []Vector\n</code></pre> <p>Generates <code>n</code> random vectors of dimension <code>dim</code> using the given random seed.</p> <p>Parameters: - <code>n</code>: Number of vectors to generate - <code>dim</code>: Dimension of each vector - <code>seed</code>: Random seed for reproducibility</p> <p>Returns: Slice of random vectors with sequential IDs starting from 0.</p> <p>Example:</p> <pre><code>vectors := vector.GenerateRandom(1000, 128, 42)\n</code></pre>"},{"location":"api/#validatedimension","title":"ValidateDimension","text":"<pre><code>func ValidateDimension(vs []Vector, dim int) error\n</code></pre> <p>Validates that all vectors have the expected dimension.</p> <p>Parameters: - <code>vs</code>: Slice of vectors to validate - <code>dim</code>: Expected dimension</p> <p>Returns: Error if any vector has incorrect dimension.</p>"},{"location":"api/#copy","title":"Copy","text":"<pre><code>func Copy(s []float32) []float32\n</code></pre> <p>Returns a deep copy of a float32 slice.</p>"},{"location":"api/#add","title":"Add","text":"<pre><code>func Add(a, b []float32) []float32\n</code></pre> <p>Element-wise addition of two vectors.</p>"},{"location":"api/#subtract","title":"Subtract","text":"<pre><code>func Subtract(a, b []float32) []float32\n</code></pre> <p>Element-wise subtraction (a - b).</p>"},{"location":"api/#scale","title":"Scale","text":"<pre><code>func Scale(v []float32, s float32) []float32\n</code></pre> <p>Multiplies vector by scalar.</p>"},{"location":"api/#norm","title":"Norm","text":"<pre><code>func Norm(v []float32) float32\n</code></pre> <p>Computes L2 norm (magnitude) of a vector.</p> <p>Example:</p> <pre><code>magnitude := vector.Norm(myVector)\n</code></pre>"},{"location":"api/#normalize","title":"Normalize","text":"<pre><code>func Normalize(v []float32) []float32\n</code></pre> <p>Returns normalized vector (unit length). Returns original if norm is zero.</p>"},{"location":"api/#normalizeinplace","title":"NormalizeInPlace","text":"<pre><code>func NormalizeInPlace(v []float32)\n</code></pre> <p>Normalizes vector in-place to unit length.</p> <p>Example:</p> <pre><code>vector.NormalizeInPlace(myVector)\n</code></pre>"},{"location":"api/#centroid","title":"Centroid","text":"<pre><code>func Centroid(vectors [][]float32) []float32\n</code></pre> <p>Computes the mean (centroid) of a set of vectors.</p>"},{"location":"api/#metric-package","title":"Metric Package","text":"<p><code>import \"github.com/tahcohcat/gofaiss/pkg/metric\"</code></p>"},{"location":"api/#types_1","title":"Types","text":""},{"location":"api/#type","title":"Type","text":"<pre><code>type Type string\n\nconst (\n    L2     Type = \"l2\"     // Euclidean distance\n    Cosine Type = \"cosine\" // Cosine similarity\n    Dot    Type = \"dot\"    // Inner product\n)\n</code></pre> <p>Supported distance metrics.</p>"},{"location":"api/#metric","title":"Metric","text":"<pre><code>type Metric interface {\n    Distance(a, b []float32) float32\n    Name() string\n}\n</code></pre> <p>Interface for distance computations.</p>"},{"location":"api/#functions_1","title":"Functions","text":""},{"location":"api/#new","title":"New","text":"<pre><code>func New(t Type) (Metric, error)\n</code></pre> <p>Creates a new metric instance.</p> <p>Parameters: - <code>t</code>: Metric type (L2, Cosine, or Dot)</p> <p>Returns: Metric instance or error for unknown type.</p> <p>Example:</p> <pre><code>m, err := metric.New(metric.L2)\nif err != nil {\n    log.Fatal(err)\n}\ndist := m.Distance(vec1, vec2)\n</code></pre>"},{"location":"api/#index-types","title":"Index Types","text":""},{"location":"api/#common-interface","title":"Common Interface","text":"<p>All indexes implement:</p> <pre><code>type Index interface {\n    Add(vectors []Vector) error\n    Search(query []float32, k int) ([]SearchResult, error)\n    BatchSearch(queries [][]float32, k int) ([][]SearchResult, error)\n    Dimension() int\n    Stats() stats.Stats\n}\n</code></pre>"},{"location":"api/#flat-index","title":"Flat Index","text":"<p><code>import \"github.com/tahcohcat/gofaiss/pkg/index/flat\"</code></p> <p>Exact brute-force search index. Guarantees 100% recall.</p>"},{"location":"api/#types_2","title":"Types","text":"<pre><code>type Index struct {\n    // Private fields\n}\n</code></pre>"},{"location":"api/#functions_2","title":"Functions","text":""},{"location":"api/#new_1","title":"New","text":"<pre><code>func New(dim int, metric string) (*Index, error)\n</code></pre> <p>Creates a new flat index.</p> <p>Parameters: - <code>dim</code>: Vector dimension (must be &gt; 0) - <code>metric</code>: Distance metric (\"l2\" or \"cosine\")</p> <p>Returns: Index instance or error.</p> <p>Example:</p> <pre><code>idx, err := flat.New(128, \"l2\")\nif err != nil {\n    log.Fatal(err)\n}\n</code></pre>"},{"location":"api/#methods","title":"Methods","text":""},{"location":"api/#add_1","title":"Add","text":"<pre><code>func (idx *Index) Add(vs []Vector) error\n</code></pre> <p>Adds vectors to the index.</p> <p>Parameters: - <code>vs</code>: Vectors to add</p> <p>Returns: Error if dimension mismatch or zero vector with cosine metric.</p> <p>Concurrency: Thread-safe.</p>"},{"location":"api/#search","title":"Search","text":"<pre><code>func (idx *Index) Search(q []float32, k int) ([]SearchResult, error)\n</code></pre> <p>Searches for k nearest neighbors.</p> <p>Parameters: - <code>q</code>: Query vector - <code>k</code>: Number of neighbors to return</p> <p>Returns: Sorted results (closest first) or error.</p> <p>Concurrency: Thread-safe (read lock).</p>"},{"location":"api/#batchsearch","title":"BatchSearch","text":"<pre><code>func (idx *Index) BatchSearch(queries [][]float32, k int) ([][]SearchResult, error)\n</code></pre> <p>Performs batch search for multiple queries.</p>"},{"location":"api/#dimension","title":"Dimension","text":"<pre><code>func (idx *Index) Dimension() int\n</code></pre> <p>Returns vector dimension.</p>"},{"location":"api/#getvectors","title":"GetVectors","text":"<pre><code>func (idx *Index) GetVectors() []Vector\n</code></pre> <p>Returns all stored vectors.</p>"},{"location":"api/#stats","title":"Stats","text":"<pre><code>func (idx *Index) Stats() stats.Stats\n</code></pre> <p>Returns index statistics.</p>"},{"location":"api/#saveload","title":"Save/Load","text":"<pre><code>func (idx *Index) Save(w storage.Writer) error\nfunc (idx *Index) Load(r storage.Reader) error\n</code></pre> <p>Serialization methods.</p>"},{"location":"api/#hnsw-index","title":"HNSW Index","text":"<p><code>import \"github.com/tahcohcat/gofaiss/pkg/index/hnsw\"</code></p> <p>Hierarchical Navigable Small World graph index. Fast approximate search with high recall.</p>"},{"location":"api/#types_3","title":"Types","text":""},{"location":"api/#config","title":"Config","text":"<pre><code>type Config struct {\n    Metric         string // Distance metric\n    M              int    // Connections per layer\n    EfConstruction int    // Build quality\n    EfSearch       int    // Search quality\n}\n</code></pre> <p>HNSW configuration parameters.</p> <p>Field Details: - <code>M</code>: Number of bi-directional links per node (typical: 16-64)   - Higher = better recall, more memory - <code>EfConstruction</code>: Candidate list size during construction (typical: 100-500)   - Higher = better quality, slower build - <code>EfSearch</code>: Candidate list size during search (typical: 50-500)   - Can be adjusted at runtime</p>"},{"location":"api/#node","title":"Node","text":"<pre><code>type Node struct {\n    ID    int64\n    Data  []float32\n    Level int\n    Edges [][]int64 // Edges per level\n}\n</code></pre> <p>Internal graph node structure.</p>"},{"location":"api/#functions_3","title":"Functions","text":""},{"location":"api/#new_2","title":"New","text":"<pre><code>func New(dim int, metricType string, config Config) (*Index, error)\n</code></pre> <p>Creates a new HNSW index.</p> <p>Parameters: - <code>dim</code>: Vector dimension - <code>metricType</code>: Distance metric (\"l2\", \"cosine\", or \"dot\") - <code>config</code>: HNSW configuration</p> <p>Returns: Index instance or error.</p> <p>Example:</p> <pre><code>config := hnsw.Config{\n    M:              16,\n    EfConstruction: 200,\n    EfSearch:       50,\n}\nidx, err := hnsw.New(128, \"l2\", config)\n</code></pre>"},{"location":"api/#defaultconfig","title":"DefaultConfig","text":"<pre><code>func DefaultConfig() Config\n</code></pre> <p>Returns default HNSW configuration.</p> <p>Returns:</p> <pre><code>Config{\n    Metric:         \"l2\",\n    M:              16,\n    EfConstruction: 200,\n    EfSearch:       200,\n}\n</code></pre>"},{"location":"api/#methods_1","title":"Methods","text":""},{"location":"api/#add_2","title":"Add","text":"<pre><code>func (idx *Index) Add(vectors []Vector) error\n</code></pre> <p>Adds vectors to the HNSW graph.</p> <p>Parameters: - <code>vectors</code>: Vectors to add</p> <p>Returns: Error if dimension mismatch.</p> <p>Concurrency: Thread-safe (write lock).</p> <p>Note: Auto-generates IDs if Vector.ID is 0.</p>"},{"location":"api/#search_1","title":"Search","text":"<pre><code>func (idx *Index) Search(query []float32, k int) ([]SearchResult, error)\n</code></pre> <p>Searches for k nearest neighbors using HNSW algorithm.</p> <p>Parameters: - <code>query</code>: Query vector - <code>k</code>: Number of neighbors</p> <p>Returns: Approximately k nearest neighbors, sorted by distance.</p> <p>Concurrency: Thread-safe (read lock).</p> <p>Complexity: O(log N) average case.</p>"},{"location":"api/#batchsearch_1","title":"BatchSearch","text":"<pre><code>func (idx *Index) BatchSearch(queries [][]float32, k int) ([][]SearchResult, error)\n</code></pre> <p>Batch search for multiple queries.</p>"},{"location":"api/#setefsearch","title":"SetEfSearch","text":"<pre><code>func (idx *Index) SetEfSearch(ef int)\n</code></pre> <p>Adjusts search quality parameter at runtime.</p> <p>Parameters: - <code>ef</code>: New efSearch value (higher = better recall, slower)</p> <p>Example:</p> <pre><code>idx.SetEfSearch(100) // Increase search quality\n</code></pre>"},{"location":"api/#remove","title":"Remove","text":"<pre><code>func (idx *Index) Remove(id int64) error\n</code></pre> <p>Removes a vector from the index.</p> <p>Parameters: - <code>id</code>: Vector ID to remove</p> <p>Returns: Error if vector not found.</p> <p>Note: Expensive operation, rebuilds connections.</p>"},{"location":"api/#size","title":"Size","text":"<pre><code>func (idx *Index) Size() int\n</code></pre> <p>Returns number of vectors in index.</p>"},{"location":"api/#dimension_1","title":"Dimension","text":"<pre><code>func (idx *Index) Dimension() int\n</code></pre> <p>Returns vector dimension.</p>"},{"location":"api/#stats_1","title":"Stats","text":"<pre><code>func (idx *Index) Stats() stats.Stats\n</code></pre> <p>Returns detailed statistics including: - Total vectors - Memory usage - Max graph level - Configuration parameters</p> <p>Example:</p> <pre><code>stats := idx.Stats()\nfmt.Printf(\"Max level: %v\\n\", stats.ExtraInfo[\"maxLevel\"])\n</code></pre>"},{"location":"api/#ivf-index","title":"IVF Index","text":"<p><code>import \"github.com/tahcohcat/gofaiss/pkg/index/ivf\"</code></p> <p>Inverted File index using k-means clustering. Balanced speed and memory.</p>"},{"location":"api/#types_4","title":"Types","text":""},{"location":"api/#config_1","title":"Config","text":"<pre><code>type Config struct {\n    Metric string // Distance metric\n    Nlist  int    // Number of clusters\n}\n</code></pre> <p>IVF configuration.</p> <p>Field Details: - <code>Nlist</code>: Number of Voronoi cells (typical: sqrt(N), range: [100, 65536])</p>"},{"location":"api/#functions_4","title":"Functions","text":""},{"location":"api/#new_3","title":"New","text":"<pre><code>func New(dim int, metricType string, config Config) (*Index, error)\n</code></pre> <p>Creates a new IVF index.</p> <p>Example:</p> <pre><code>config := ivf.Config{\n    Metric: \"l2\",\n    Nlist:  1000,\n}\nidx, err := ivf.New(128, \"l2\", config)\n</code></pre>"},{"location":"api/#defaultconfig_1","title":"DefaultConfig","text":"<pre><code>func DefaultConfig(numVectors int) Config\n</code></pre> <p>Returns default configuration based on dataset size.</p> <p>Parameters: - <code>numVectors</code>: Expected number of vectors</p> <p>Returns: Config with <code>nlist = sqrt(numVectors)</code>, clamped to [10, 65536].</p>"},{"location":"api/#methods_2","title":"Methods","text":""},{"location":"api/#train","title":"Train","text":"<pre><code>func (idx *Index) Train(vectors []Vector) error\n</code></pre> <p>Trains the index by clustering vectors into cells.</p> <p>Parameters: - <code>vectors</code>: Training vectors (need at least <code>nlist</code> vectors)</p> <p>Returns: Error if insufficient training data.</p> <p>Required: Must be called before <code>Add()</code>.</p> <p>Example:</p> <pre><code>err := idx.Train(trainingVectors)\nif err != nil {\n    log.Fatal(err)\n}\n</code></pre>"},{"location":"api/#istrained","title":"IsTrained","text":"<pre><code>func (idx *Index) IsTrained() bool\n</code></pre> <p>Returns whether index has been trained.</p>"},{"location":"api/#add_3","title":"Add","text":"<pre><code>func (idx *Index) Add(vectors []Vector) error\n</code></pre> <p>Adds vectors to the index (assigns to nearest cluster).</p> <p>Returns: Error if index not trained.</p>"},{"location":"api/#search_2","title":"Search","text":"<pre><code>func (idx *Index) Search(query []float32, k int) ([]SearchResult, error)\n</code></pre> <p>Searches using inverted file algorithm.</p> <p>Note: Uses <code>nprobe</code> parameter to determine how many clusters to search.</p>"},{"location":"api/#setnprobe","title":"SetNProbe","text":"<pre><code>func (idx *Index) SetNProbe(nprobe int)\n</code></pre> <p>Sets number of clusters to search.</p> <p>Parameters: - <code>nprobe</code>: Number of nearest clusters to search (1-nlist)   - Higher = better recall, slower search   - Typical: 1-20</p> <p>Example:</p> <pre><code>idx.SetNProbe(10) // Search 10 nearest clusters\n</code></pre>"},{"location":"api/#batchsearch_2","title":"BatchSearch","text":"<pre><code>func (idx *Index) BatchSearch(queries [][]float32, k int) ([][]SearchResult, error)\n</code></pre> <p>Batch search for multiple queries.</p>"},{"location":"api/#dimension_2","title":"Dimension","text":"<pre><code>func (idx *Index) Dimension() int\n</code></pre> <p>Returns vector dimension.</p>"},{"location":"api/#stats_2","title":"Stats","text":"<pre><code>func (idx *Index) Stats() stats.Stats\n</code></pre> <p>Returns statistics including cluster distribution.</p>"},{"location":"api/#pq-index","title":"PQ Index","text":"<p><code>import \"github.com/tahcohcat/gofaiss/pkg/index/pq\"</code></p> <p>Product Quantization index for memory-efficient vector storage.</p>"},{"location":"api/#types_5","title":"Types","text":""},{"location":"api/#config_2","title":"Config","text":"<pre><code>type Config struct {\n    M     int // Number of subquantizers\n    Nbits int // Bits per subquantizer\n}\n</code></pre> <p>PQ configuration.</p> <p>Field Details: - <code>M</code>: Number of subquantizers (must divide dimension evenly, typical: 8-64) - <code>Nbits</code>: Bits per code (typical: 8, meaning 256 centroids)   - Memory: ~M bytes per vector   - Compression: ~(dimension * 4) / M</p>"},{"location":"api/#functions_5","title":"Functions","text":""},{"location":"api/#newindex","title":"NewIndex","text":"<pre><code>func NewIndex(dim int, cfg Config) (*Index, error)\n</code></pre> <p>Creates a new PQ index.</p> <p>Parameters: - <code>dim</code>: Vector dimension (must be divisible by M) - <code>cfg</code>: PQ configuration</p> <p>Returns: Index instance or error.</p> <p>Example:</p> <pre><code>config := pq.Config{\n    M:     16,  // 16 subquantizers\n    Nbits: 8,   // 256 centroids each\n}\nidx, err := pq.NewIndex(128, config)\n</code></pre>"},{"location":"api/#methods_3","title":"Methods","text":""},{"location":"api/#train_1","title":"Train","text":"<pre><code>func (idx *Index) Train(vectors []Vector) error\n</code></pre> <p>Trains PQ codebooks using k-means on subspaces.</p> <p>Parameters: - <code>vectors</code>: Training vectors (need at least 2^Nbits vectors)</p> <p>Returns: Error if insufficient training data or dimension mismatch.</p> <p>Required: Must be called before <code>Add()</code>.</p> <p>Example:</p> <pre><code>trainingVectors := vector.GenerateRandom(5000, 128, 42)\nerr := idx.Train(trainingVectors)\n</code></pre>"},{"location":"api/#add_4","title":"Add","text":"<pre><code>func (idx *Index) Add(vectors []Vector) error\n</code></pre> <p>Adds vectors by encoding them with PQ.</p> <p>Returns: Error if index not trained.</p> <p>Note: Vectors are automatically compressed using learned codebooks.</p>"},{"location":"api/#search_3","title":"Search","text":"<pre><code>func (idx *Index) Search(query []float32, k int) ([]SearchResult, error)\n</code></pre> <p>Searches using asymmetric distance computation.</p> <p>Note: Uses full-precision query against compressed database.</p>"},{"location":"api/#batchsearch_3","title":"BatchSearch","text":"<pre><code>func (idx *Index) BatchSearch(queries [][]float32, k int) ([][]SearchResult, error)\n</code></pre> <p>Batch search for multiple queries.</p>"},{"location":"api/#dimension_3","title":"Dimension","text":"<pre><code>func (idx *Index) Dimension() int\n</code></pre> <p>Returns vector dimension.</p>"},{"location":"api/#stats_3","title":"Stats","text":"<pre><code>func (idx *Index) Stats() stats.Stats\n</code></pre> <p>Returns statistics including compression ratio.</p> <p>Example:</p> <pre><code>stats := idx.Stats()\ncompressionRatio := float64(stats.TotalVectors * dim * 4) / (stats.MemoryUsageMB * 1024 * 1024)\nfmt.Printf(\"Compression: %.1fx\\n\", compressionRatio)\n</code></pre>"},{"location":"api/#ivfpq-index","title":"IVFPQ Index","text":"<p><code>import \"github.com/tahcohcat/gofaiss/pkg/index/ivfpq\"</code></p> <p>Combines IVF and PQ for fast search with compression. Best for large-scale datasets.</p>"},{"location":"api/#types_6","title":"Types","text":""},{"location":"api/#config_3","title":"Config","text":"<pre><code>type Config struct {\n    Metric string // Distance metric\n    Nlist  int    // Number of IVF clusters\n    M      int    // PQ subquantizers\n    Nbits  int    // PQ bits per code\n}\n</code></pre> <p>IVFPQ configuration combining IVF and PQ parameters.</p>"},{"location":"api/#functions_6","title":"Functions","text":""},{"location":"api/#new_4","title":"New","text":"<pre><code>func New(dim int, metricType string, config Config) (*Index, error)\n</code></pre> <p>Creates a new IVFPQ index.</p> <p>Example:</p> <pre><code>config := ivfpq.Config{\n    Metric: \"l2\",\n    Nlist:  1000,  // IVF clusters\n    M:      16,    // PQ subquantizers\n    Nbits:  8,     // PQ bits\n}\nidx, err := ivfpq.New(128, \"l2\", config)\n</code></pre>"},{"location":"api/#methods_4","title":"Methods","text":"<p>Similar to IVF Index but with additional PQ compression:</p> <ul> <li><code>Train()</code>: Trains both IVF clustering and PQ codebooks</li> <li><code>Add()</code>: Assigns to clusters and compresses</li> <li><code>Search()</code>: Fast approximate search with compression</li> <li><code>SetNProbe()</code>: Adjusts search quality</li> </ul>"},{"location":"api/#storage-package","title":"Storage Package","text":"<p><code>import \"github.com/tahcohcat/gofaiss/pkg/storage\"</code></p> <p>Handles index persistence.</p>"},{"location":"api/#functions_7","title":"Functions","text":""},{"location":"api/#saveindex","title":"SaveIndex","text":"<pre><code>func SaveIndex(idx interface{}, filename string, compress bool) error\n</code></pre> <p>Saves an index to a file.</p> <p>Parameters: - <code>idx</code>: Index to save (must implement Save method) - <code>filename</code>: Output file path - <code>compress</code>: Enable gzip compression</p> <p>Example:</p> <pre><code>err := storage.SaveIndex(idx, \"index.faiss.gz\", true)\n</code></pre>"},{"location":"api/#loadindex","title":"LoadIndex","text":"<pre><code>func LoadIndex(idx interface{}, filename string, compress bool) error\n</code></pre> <p>Loads an index from a file.</p> <p>Parameters: - <code>idx</code>: Index to load into (must implement Load method) - <code>filename</code>: Input file path - <code>compress</code>: File is gzip compressed</p> <p>Example:</p> <pre><code>idx := &amp;hnsw.Index{}\nerr := storage.LoadIndex(idx, \"index.faiss.gz\", true)\n</code></pre>"},{"location":"api/#interfaces","title":"Interfaces","text":""},{"location":"api/#writer","title":"Writer","text":"<pre><code>type Writer interface {\n    Encode(v interface{}) error\n}\n</code></pre> <p>Interface for encoding data.</p>"},{"location":"api/#reader","title":"Reader","text":"<pre><code>type Reader interface {\n    Decode(v interface{}) error\n}\n</code></pre> <p>Interface for decoding data.</p>"},{"location":"api/#stats-package","title":"Stats Package","text":"<p><code>import \"github.com/tahcohcat/gofaiss/pkg/index/stats\"</code></p> <p>Provides index statistics.</p>"},{"location":"api/#types_7","title":"Types","text":""},{"location":"api/#stats_4","title":"Stats","text":"<pre><code>type Stats struct {\n    TotalVectors  int                    // Number of vectors\n    Dimension     int                    // Vector dimension\n    IndexType     string                 // Index type name\n    MemoryUsageMB float64                // Estimated memory (MB)\n    ExtraInfo     map[string]interface{} // Type-specific info\n}\n</code></pre> <p>Index statistics structure.</p> <p>ExtraInfo Examples:</p> <p>HNSW:</p> <pre><code>{\n    \"metric\": \"l2\",\n    \"M\": 16,\n    \"efConstruction\": 200,\n    \"efSearch\": 50,\n    \"maxLevel\": 4,\n}\n</code></pre> <p>IVF:</p> <pre><code>{\n    \"metric\": \"l2\",\n    \"nlist\": 1000,\n    \"nprobe\": 10,\n    \"clusterSizes\": [...],\n}\n</code></pre>"},{"location":"api/#complete-usage-example","title":"Complete Usage Example","text":"<pre><code>package main\n\nimport (\n    \"fmt\"\n    \"log\"\n\n    \"github.com/tahcohcat/gofaiss/pkg/index/hnsw\"\n    \"github.com/tahcohcat/gofaiss/pkg/metric\"\n    \"github.com/tahcohcat/gofaiss/pkg/storage\"\n    \"github.com/tahcohcat/gofaiss/pkg/vector\"\n)\n\nfunc main() {\n    // Create index\n    dim := 128\n    config := hnsw.Config{\n        M:              16,\n        EfConstruction: 200,\n        EfSearch:       50,\n    }\n\n    idx, err := hnsw.New(dim, \"l2\", config)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    // Generate and add vectors\n    vectors := vector.GenerateRandom(10000, dim, 42)\n    if err := idx.Add(vectors); err != nil {\n        log.Fatal(err)\n    }\n\n    // Search\n    query := make([]float32, dim)\n    results, err := idx.Search(query, 10)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    // Print results\n    for i, r := range results {\n        fmt.Printf(\"%d: ID=%d, Dist=%.4f\\n\", i+1, r.ID, r.Distance)\n    }\n\n    // Save index\n    if err := storage.SaveIndex(idx, \"index.faiss.gz\", true); err != nil {\n        log.Fatal(err)\n    }\n\n    // Statistics\n    stats := idx.Stats()\n    fmt.Printf(\"\\nStats:\\n\")\n    fmt.Printf(\"Vectors: %d\\n\", stats.TotalVectors)\n    fmt.Printf(\"Memory: %.2f MB\\n\", stats.MemoryUsageMB)\n}\n</code></pre>"},{"location":"api/#error-handling","title":"Error Handling","text":"<p>All methods that can fail return errors. Common error scenarios:</p> <ul> <li>Dimension mismatch: Query or vector dimension doesn't match index</li> <li>Untrained index: Adding to IVF/PQ/IVFPQ before training</li> <li>Invalid configuration: Negative dimensions, invalid metrics</li> <li>Not found: Removing non-existent vector</li> <li>I/O errors: File save/load failures</li> </ul> <p>Always check errors:</p> <pre><code>if err := idx.Add(vectors); err != nil {\n    log.Fatalf(\"Add failed: %v\", err)\n}\n</code></pre>"},{"location":"api/#concurrency","title":"Concurrency","text":"<p>All index types are thread-safe:</p> <ul> <li>Multiple goroutines can call <code>Search()</code> concurrently</li> <li><code>Add()</code> and other write operations use write locks</li> <li>Read operations use read locks for better performance</li> </ul> <p>Example:</p> <pre><code>// Safe concurrent search\nvar wg sync.WaitGroup\nfor i := 0; i &lt; 10; i++ {\n    wg.Add(1)\n    go func(query []float32) {\n        defer wg.Done()\n        results, _ := idx.Search(query, 10)\n        // Process results\n    }(queries[i])\n}\nwg.Wait()\n</code></pre>"},{"location":"architecture/","title":"Architecture","text":"<p>Deep dive into GoFAISS implementation details, algorithms, and design decisions.</p>"},{"location":"architecture/#system-architecture","title":"System Architecture","text":""},{"location":"architecture/#high-level-overview","title":"High-Level Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Application Layer                    \u2502\n\u2502  (User code using GoFAISS API)                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  Public API Layer                        \u2502\n\u2502  pkg/index/    - Index implementations                   \u2502\n\u2502  pkg/vector/   - Vector operations                       \u2502\n\u2502  pkg/search/   - High-level search API                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  Core Components                         \u2502\n\u2502  pkg/metric/   - Distance calculations                   \u2502\n\u2502  pkg/storage/  - Persistence and serialization          \u2502\n\u2502  internal/     - Internal utilities                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#package-structure","title":"Package Structure","text":"<pre><code>gofaiss/\n\u251c\u2500\u2500 pkg/                    # Public API\n\u2502   \u251c\u2500\u2500 metric/            # Distance metrics (L2, cosine, IP)\n\u2502   \u2502   \u251c\u2500\u2500 l2.go\n\u2502   \u2502   \u251c\u2500\u2500 cosine.go\n\u2502   \u2502   \u2514\u2500\u2500 inner_product.go\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 index/             # Index implementations\n\u2502   \u2502   \u251c\u2500\u2500 flat/          # Brute-force index\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 index.go\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 search.go\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 hnsw/          # HNSW implementation\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 index.go\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 graph.go\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 search.go\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 build.go\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 ivf/           # Inverted file index\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 index.go\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 clustering.go\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 search.go\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 pq/            # Product quantization\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 index.go\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 codebook.go\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 quantize.go\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 ivfpq/         # IVF + PQ combined\n\u2502   \u2502       \u251c\u2500\u2500 index.go\n\u2502   \u2502       \u2514\u2500\u2500 search.go\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 vector/            # Vector utilities\n\u2502   \u2502   \u251c\u2500\u2500 vector.go      # Vector type definition\n\u2502   \u2502   \u251c\u2500\u2500 operations.go  # Vector math\n\u2502   \u2502   \u2514\u2500\u2500 generator.go   # Synthetic data\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 storage/           # Persistence\n\u2502   \u2502   \u251c\u2500\u2500 serializer.go  # Gob/JSON serialization\n\u2502   \u2502   \u2514\u2500\u2500 compression.go # Gzip support\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 search/            # High-level API\n\u2502       \u2514\u2500\u2500 api.go         # Unified search interface\n\u2502\n\u251c\u2500\u2500 internal/              # Internal packages\n\u2502   \u251c\u2500\u2500 math/             # Low-level math operations\n\u2502   \u2502   \u251c\u2500\u2500 simd.go       # SIMD optimizations (future)\n\u2502   \u2502   \u2514\u2500\u2500 distance.go   # Optimized distance functions\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 storage/          # Internal storage utilities\n\u2502   \u2502   \u2514\u2500\u2500 buffer.go     # Memory buffers\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 utils/            # Helper functions\n\u2502       \u251c\u2500\u2500 heap.go       # Priority queue\n\u2502       \u2514\u2500\u2500 sort.go       # Sorting utilities\n\u2502\n\u251c\u2500\u2500 cmd/                  # Command-line tools\n\u2502   \u251c\u2500\u2500 gofaiss-cli/     # CLI interface\n\u2502   \u2502   \u2514\u2500\u2500 main.go\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 gofaiss-server/  # HTTP server\n\u2502       \u2514\u2500\u2500 main.go\n\u2502\n\u251c\u2500\u2500 benchmark/            # Benchmarking suite\n\u2502   \u251c\u2500\u2500 benchmark_comparison.go\n\u2502   \u251c\u2500\u2500 run_benchmark.sh\n\u2502   \u2514\u2500\u2500 scripts/\n\u2502\n\u2514\u2500\u2500 examples/             # Usage examples\n    \u251c\u2500\u2500 basic/\n    \u251c\u2500\u2500 batch/\n    \u2514\u2500\u2500 persistence/\n</code></pre>"},{"location":"architecture/#core-algorithms","title":"Core Algorithms","text":""},{"location":"architecture/#1-flat-index-brute-force","title":"1. Flat Index (Brute Force)","text":"<p>Algorithm: Linear scan with distance computation</p> <pre><code>function Search(query, k):\n    distances = []\n    for each vector in database:\n        d = distance(query, vector)\n        distances.append((d, vector.id))\n\n    sort(distances)\n    return top k results\n</code></pre> <p>Complexity: - Time: O(n \u00d7 d) where n=vectors, d=dimensions - Space: O(n \u00d7 d)</p> <p>Implementation Details: - Uses optimized distance functions - Preallocates result buffers - Sorts results using heap for k-selection</p> <p>Code Structure:</p> <pre><code>type FlatIndex struct {\n    vectors    []vector.Vector\n    dimension  int\n    metric     string\n    distanceFunc func([]float32, []float32) float32\n}\n\nfunc (idx *FlatIndex) Search(query []float32, k int) ([]SearchResult, error) {\n    // Compute all distances\n    distances := make([]SearchResult, len(idx.vectors))\n    for i, v := range idx.vectors {\n        distances[i] = SearchResult{\n            ID:       v.ID,\n            Distance: idx.distanceFunc(query, v.Data),\n        }\n    }\n\n    // k-select using partial sort\n    partialSort(distances, k)\n    return distances[:k], nil\n}\n</code></pre>"},{"location":"architecture/#2-hnsw-hierarchical-navigable-small-world","title":"2. HNSW (Hierarchical Navigable Small World)","text":"<p>Algorithm: Multi-layer navigable graph</p> <pre><code>Build Phase:\n    for each vector v:\n        level = randomLevel()  // Exponential decay\n        for l from level down to 0:\n            neighbors = searchLayer(v, M, l)\n            connect(v, neighbors, l)\n            pruneEdges(v, M, l)\n\nSearch Phase:\n    entry = topLayerNode\n    for l from topLayer down to 0:\n        entry = searchLayer(query, entry, efSearch, l)\n    return k nearest from entry\n</code></pre> <p>Complexity: - Build: O(n \u00d7 log(n) \u00d7 M \u00d7 d) - Search: O(log(n) \u00d7 M \u00d7 d) - Space: O(n \u00d7 M \u00d7 log(n))</p> <p>Key Components:</p> <p>1. Graph Structure:</p> <pre><code>type HNSWIndex struct {\n    graphs     [][]Graph      // graphs[level][nodeID] = neighbors\n    entryPoint int64\n    maxLevel   int\n    vectors    map[int64]vector.Vector\n\n    // Parameters\n    M              int  // Max connections per node\n    EfConstruction int  // Search depth during build\n    EfSearch       int  // Search depth during query\n}\n\ntype Graph struct {\n    Neighbors []int64  // IDs of connected nodes\n}\n</code></pre> <p>2. Level Assignment:</p> <pre><code>func (idx *HNSWIndex) randomLevel() int {\n    // Exponential decay: P(level) = (1/ln(M)) ^ level\n    mL := 1.0 / math.Log(float64(idx.M))\n    r := rand.Float64()\n    return int(-math.Log(r) * mL)\n}\n</code></pre> <p>3. Search Layer:</p> <pre><code>func (idx *HNSWIndex) searchLayer(query []float32, entry int64, \n                                   ef int, layer int) []Candidate {\n    visited := make(map[int64]bool)\n    candidates := NewMaxHeap(ef)  // Keep ef best candidates\n    results := NewMinHeap(ef)\n\n    candidates.Push(entry, distance(query, idx.vectors[entry]))\n    visited[entry] = true\n\n    for !candidates.Empty() {\n        current := candidates.Pop()\n        if current.distance &gt; results.Top().distance {\n            break  // No improvements possible\n        }\n\n        for _, neighborID := range idx.graphs[layer][current.id].Neighbors {\n            if visited[neighborID] {\n                continue\n            }\n            visited[neighborID] = true\n\n            d := distance(query, idx.vectors[neighborID])\n            if d &lt; results.Top().distance || results.Size() &lt; ef {\n                candidates.Push(neighborID, d)\n                results.Push(neighborID, d)\n                if results.Size() &gt; ef {\n                    results.Pop()  // Remove worst\n                }\n            }\n        }\n    }\n\n    return results.ToSortedSlice()\n}\n</code></pre> <p>4. Edge Pruning (Heuristic):</p> <pre><code>func (idx *HNSWIndex) pruneEdges(nodeID int64, M int, layer int) {\n    neighbors := idx.graphs[layer][nodeID].Neighbors\n    if len(neighbors) &lt;= M {\n        return\n    }\n\n    // Keep M closest neighbors\n    sort.Slice(neighbors, func(i, j int) bool {\n        di := distance(idx.vectors[nodeID], idx.vectors[neighbors[i]])\n        dj := distance(idx.vectors[nodeID], idx.vectors[neighbors[j]])\n        return di &lt; dj\n    })\n\n    idx.graphs[layer][nodeID].Neighbors = neighbors[:M]\n}\n</code></pre> <p>Design Decisions: - Used map for sparse graph storage (memory efficient) - Separate graphs per layer for clarity - Preallocate heaps to reduce allocations - Lock-free reads, write locks only during insertion</p>"},{"location":"architecture/#3-ivf-inverted-file-index","title":"3. IVF (Inverted File Index)","text":"<p>Algorithm: Clustering-based partitioning</p> <pre><code>Training Phase:\n    centroids = kmeans(training_vectors, nlist)\n\nBuild Phase:\n    for each vector v:\n        nearest_cluster = argmin(distance(v, centroids))\n        inverted_lists[nearest_cluster].append(v)\n\nSearch Phase:\n    nearest_clusters = topK(query, centroids, nprobe)\n    candidates = []\n    for cluster in nearest_clusters:\n        candidates.extend(inverted_lists[cluster])\n    return topK(query, candidates, k)\n</code></pre> <p>Complexity: - Train: O(iterations \u00d7 nlist \u00d7 training_size \u00d7 d) - Build: O(n \u00d7 nlist \u00d7 d) - Search: O(nprobe \u00d7 (n/nlist) \u00d7 d) - Space: O(n \u00d7 d + nlist \u00d7 d)</p> <p>Implementation:</p> <pre><code>type IVFIndex struct {\n    centroids      [][]float32           // [nlist][dimension]\n    invertedLists  [][]vector.Vector     // [nlist][vectors]\n    nlist          int\n    metric         string\n    trained        bool\n}\n\nfunc (idx *IVFIndex) Train(vectors []vector.Vector) error {\n    // K-means clustering\n    idx.centroids = kmeans(vectors, idx.nlist, maxIterations)\n    idx.trained = true\n    return nil\n}\n\nfunc (idx *IVFIndex) Add(vectors []vector.Vector) error {\n    if !idx.trained {\n        return errors.New(\"index not trained\")\n    }\n\n    // Assign each vector to nearest centroid\n    for _, v := range vectors {\n        clusterID := idx.findNearestCentroid(v.Data)\n        idx.invertedLists[clusterID] = append(\n            idx.invertedLists[clusterID], v)\n    }\n    return nil\n}\n\nfunc (idx *IVFIndex) Search(query []float32, k, nprobe int) ([]SearchResult, error) {\n    // Find nprobe nearest clusters\n    clusterDistances := make([]ClusterDistance, idx.nlist)\n    for i, centroid := range idx.centroids {\n        clusterDistances[i] = ClusterDistance{\n            ID:       i,\n            Distance: distance(query, centroid),\n        }\n    }\n    sort.Slice(clusterDistances, ...)  // Sort by distance\n\n    // Search in top nprobe clusters\n    var candidates []SearchResult\n    for i := 0; i &lt; nprobe &amp;&amp; i &lt; len(clusterDistances); i++ {\n        clusterID := clusterDistances[i].ID\n        for _, v := range idx.invertedLists[clusterID] {\n            candidates = append(candidates, SearchResult{\n                ID:       v.ID,\n                Distance: distance(query, v.Data),\n            })\n        }\n    }\n\n    // Return top k\n    sort.Slice(candidates, ...)\n    if len(candidates) &gt; k {\n        candidates = candidates[:k]\n    }\n    return candidates, nil\n}\n</code></pre> <p>K-means Implementation:</p> <pre><code>func kmeans(vectors []vector.Vector, k int, maxIter int) [][]float32 {\n    // Initialize centroids (k-means++)\n    centroids := kmeansppInit(vectors, k)\n\n    for iter := 0; iter &lt; maxIter; iter++ {\n        // Assignment step\n        assignments := make([]int, len(vectors))\n        for i, v := range vectors {\n            assignments[i] = findNearestCentroid(v.Data, centroids)\n        }\n\n        // Update step\n        newCentroids := make([][]float32, k)\n        counts := make([]int, k)\n\n        for i, v := range vectors {\n            cluster := assignments[i]\n            if newCentroids[cluster] == nil {\n                newCentroids[cluster] = make([]float32, len(v.Data))\n            }\n            vectorAdd(newCentroids[cluster], v.Data)\n            counts[cluster]++\n        }\n\n        // Average\n        for i := range newCentroids {\n            if counts[i] &gt; 0 {\n                vectorScale(newCentroids[i], 1.0/float32(counts[i]))\n            }\n        }\n\n        // Check convergence\n        if centroidsConverged(centroids, newCentroids) {\n            break\n        }\n        centroids = newCentroids\n    }\n\n    return centroids\n}\n</code></pre>"},{"location":"architecture/#4-product-quantization-pq","title":"4. Product Quantization (PQ)","text":"<p>Algorithm: Vector compression via subspace quantization</p> <pre><code>Training Phase:\n    split vectors into M subvectors\n    for each subspace m:\n        codebook[m] = kmeans(subvectors[m], 256)  // 8-bit codes\n\nEncoding Phase:\n    for each vector v:\n        split v into M subvectors\n        for each subspace m:\n            code[m] = argmin(distance(subvector[m], codebook[m]))\n        store code (M bytes instead of M*d*4 bytes)\n\nSearch Phase:\n    precompute distance tables for query\n    for each encoded vector:\n        distance = sum(distance_tables[m][code[m]] for m in M)\n    return top k\n</code></pre> <p>Complexity: - Train: O(M \u00d7 iterations \u00d7 256 \u00d7 (n/M) \u00d7 (d/M)) - Encode: O(n \u00d7 M \u00d7 256 \u00d7 (d/M)) - Search: O(M \u00d7 256 \u00d7 (d/M) + n \u00d7 M)  // Much faster than O(n \u00d7 d) - Space: O(n \u00d7 M) bytes vs O(n \u00d7 d \u00d7 4) bytes</p> <p>Implementation: ```go type PQIndex struct {     codebooks     [][][]float32  // [M][256][dsub]     codes         [][]uint8      // [n][M]     M             int            // Number of subquantizers     dsub          int            // Dimension per subquantizer     dimension     int     vectorIDs     []int64 }</p> <p>func (idx *PQIndex) Train(vectors []vector.Vector) error {     idx.dsub = idx.dimension / idx.M     idx.codebooks = make([][][]float32, idx.M)</p> <pre><code>// Train each subquantizer independently\nfor m := 0; m &lt; idx.M; m++ {\n    // Extract subvectors for this subspace\n    subvectors := make([][]float32, len(vectors))\n    for i, v := range vectors {\n        start := m * idx.dsub\n        end := start + idx.dsub\n        subvectors[i] = v.Data[start:end]\n    }\n\n    // K-means with k=256 (8-bit codes)\n    idx.codebooks[m] = kmeans(subvectors, 256)\n}\n\nreturn nil\n</code></pre> <p>}</p> <p>func (idx *PQIndex) Add(vectors []vector.Vector) error {     for _, v := range vectors {         // Encode vector         code := make([]uint8, idx.M)         for m := 0; m &lt; idx.M; m++ {             start := m * idx.dsub             end := start + idx.dsub             subvector := v.Data[start:end]</p> <pre><code>        // Find nearest centroid in codebook\n        code[m] = idx.quantize(subvector, m)\n    }\n\n    idx.codes = append(idx.codes, code)\n    idx.vectorIDs = append(idx.vectorIDs, v.ID)\n}\nreturn nil\n</code></pre> <p>}</p> <p>func (idx *PQIndex) Search(query []float32, k int) ([]SearchResult, error) {     // Precompute distance tables     distanceTables := make([][]float32, idx.M)     for m := 0; m &lt; idx.M; m++ {         start := m * idx.dsub         end := start + idx.dsub         querySubvector := query[start:end]</p> <pre><code>    // Distance from query subvector to all centroids\n    distanceTables[m] = make([]float32, 256)\n    for j := 0; j &lt; 256; j++ {\n        distanceTables[m][j] = distance(\n            querySubvector, \n            idx.codebooks[m][j],\n        )\n    }\n}\n\n// Compute distances using lookup\nresults := make([]SearchResult, len(idx.codes))\nfor i, code := range idx.codes {\n    dist := float32(0)\n    for m := 0; m &lt; idx.M; m++ {\n        dist += distanceTables\n</code></pre>"},{"location":"benchmarks/","title":"Performance Benchmarks","text":"<p>Comprehensive performance analysis of GoFAISS across different index types, dataset sizes, and configurations.</p>"},{"location":"benchmarks/#benchmark-methodology","title":"Benchmark Methodology","text":""},{"location":"benchmarks/#test-configuration","title":"Test Configuration","text":"<p>All benchmarks were conducted with:</p> <ul> <li>Hardware: Modern x86_64 CPU, 16GB RAM</li> <li>Dataset: Randomly generated vectors with fixed seed (42) for reproducibility</li> <li>Metrics: L2 (Euclidean) distance</li> <li>Go Version: 1.21+</li> <li>Methodology: Multiple runs with statistical analysis</li> </ul>"},{"location":"benchmarks/#measured-metrics","title":"Measured Metrics","text":"<ul> <li>Build Time: Time to construct index (milliseconds)</li> <li>Query Latency: Per-query search time</li> <li>Average (mean)</li> <li>P50 (median)</li> <li>P95 (95th percentile)</li> <li>P99 (99th percentile - tail latency)</li> <li>QPS: Queries per second (throughput)</li> <li>Memory: Total memory footprint (MB)</li> <li>Recall@K: Accuracy (% of true k-nearest neighbors found)</li> </ul>"},{"location":"benchmarks/#results-10k-vectors-128-dimensions","title":"Results: 10K Vectors, 128 Dimensions","text":"Index Type Build Time Avg Query P95 QPS Memory Recall@10 Flat 0 ms 2.58 ms 9.04 ms 387 0 MB 100.0% HNSW 1,810 ms 0.06 ms 0 ms 15,996 9.77 MB 99.9% IVF 587 ms 0.34 ms 2.62 ms 2,926 4.93 MB 33.9% PQ 1,875 ms 1.66 ms 5.56 ms 604 0.28 MB 28.5% IVFPQ 2,313 ms 0.44 ms 3.02 ms 2,297 0.25 MB 8.8% <p>Key Observations (10K scale):</p> <ul> <li>HNSW: 41x faster queries than Flat with near-perfect recall</li> <li>PQ: 28x memory compression (0.28 MB vs 9.77 MB for HNSW)</li> <li>IVF: Best balance for small datasets with 8x faster queries than Flat</li> <li>Timing Precision: P50/P95 showing 0ms indicates sub-millisecond latency below measurement granularity</li> </ul>"},{"location":"benchmarks/#results-100k-vectors-128-dimensions","title":"Results: 100K Vectors, 128 Dimensions","text":"Index Type Build Time Avg Query P95 QPS Memory Recall@10 Flat 2 ms 27.89 ms 31.55 ms 36 0 MB 100.0% HNSW 18,061 ms 0.04 ms 0 ms 24,087 97.66 MB 99.96% IVF 4,572 ms 2.92 ms 6.46 ms 343 48.98 MB 26.1% PQ 4,034 ms 19.86 ms 23.27 ms 50 1.65 MB 19.7% IVFPQ 8,506 ms 1.94 ms 5.97 ms 516 1.04 MB 4.0% <p>Key Observations (100K scale):</p> <ul> <li>HNSW: 669x faster than Flat, 24K QPS with excellent recall</li> <li>IVFPQ: Best memory efficiency at 1.04 MB (94x compression vs HNSW)</li> <li>Scaling: Query times remain stable as dataset grows (sublinear complexity)</li> <li>Build Time: HNSW takes longest to build (18s) but pays off in query speed</li> </ul>"},{"location":"benchmarks/#detailed-analysis","title":"Detailed Analysis","text":""},{"location":"benchmarks/#query-performance-by-index-type","title":"Query Performance by Index Type","text":""},{"location":"benchmarks/#flat-index-brute-force","title":"Flat Index (Brute Force)","text":"<pre><code>Dataset    | Build | Avg Query | QPS   | Memory | Recall\n10K        | 0ms   | 2.58ms    | 387   | 0 MB   | 100%\n100K       | 2ms   | 27.89ms   | 36    | 0 MB   | 100%\n</code></pre> <p>Characteristics: - Linear search complexity: O(n \u00d7 d) - Query time scales linearly with dataset size - Zero overhead - just stores vectors - Perfect recall - exact search - Best for: Small datasets (&lt;10K), exact results required</p> <p>Scaling: 10x more vectors = 10.8x slower queries (expected linear)</p>"},{"location":"benchmarks/#hnsw-index-graph-based","title":"HNSW Index (Graph-Based)","text":"<pre><code>Dataset    | Build   | Avg Query | QPS    | Memory   | Recall\n10K        | 1,810ms | 0.06ms    | 15,996 | 9.77 MB  | 99.9%\n100K       | 18,061ms| 0.04ms    | 24,087 | 97.66 MB | 99.96%\n</code></pre> <p>Characteristics: - Logarithmic search complexity: O(log n) - Query time barely increases with dataset size - Memory overhead: ~1.5x raw vector data - Near-perfect recall (98-99.9%) - Best for: Production queries, speed-critical applications</p> <p>Scaling: 10x more vectors = 10x build time, queries actually get slightly faster (improved graph connectivity)</p> <p>Configuration (used in benchmarks):</p> <pre><code>M:              16   // Connections per node\nEfConstruction: 200  // Build quality\nEfSearch:       50   // Search quality\n</code></pre> <p>Tuning Guidelines: - Recall too low? Increase <code>EfSearch</code> to 100-200 - Build too slow? Decrease <code>EfConstruction</code> to 100 - More memory OK? Increase <code>M</code> to 32</p>"},{"location":"benchmarks/#ivf-index-inverted-file","title":"IVF Index (Inverted File)","text":"<pre><code>Dataset    | Build   | Avg Query | QPS   | Memory   | Recall\n10K        | 587ms   | 0.34ms    | 2,926 | 4.93 MB  | 33.9%\n100K       | 4,572ms | 2.92ms    | 343   | 48.98 MB | 26.1%\n</code></pre> <p>Characteristics: - Sublinear search: O(\u221an) with proper nprobe - Requires training phase (k-means clustering) - Memory: ~1x raw vectors + cluster data - Recall depends heavily on nprobe setting - Best for: Medium datasets, controllable speed/accuracy tradeoff</p> <p>Scaling: 10x more vectors = 8x build time, 8.6x slower queries (sublinear as expected)</p> <p>Configuration (used in benchmarks):</p> <pre><code>nlist:  316 (100K), 100 (10K)  // \u221anumVectors\nnprobe: 10                       // Clusters to search\n</code></pre> <p>Low Recall Issue: The 26-33% recall in benchmarks is due to conservative <code>nprobe=10</code>. </p> <p>Improving Recall:</p> <pre><code>// For 90%+ recall with 100K vectors:\nresults, _ := idx.Search(query, k, 50)  // nprobe=50\n\n// For 95%+ recall:\nresults, _ := idx.Search(query, k, 100) // nprobe=100\n</code></pre> <p>Trade-off: Higher nprobe = better recall but slower queries.</p>"},{"location":"benchmarks/#pq-index-product-quantization","title":"PQ Index (Product Quantization)","text":"<pre><code>Dataset    | Build   | Avg Query | QPS | Memory  | Recall\n10K        | 1,875ms | 1.66ms    | 604 | 0.28 MB | 28.5%\n100K       | 4,034ms | 19.86ms   | 50  | 1.65 MB | 19.7%\n</code></pre> <p>Characteristics: - Compressed storage: 32x smaller than uncompressed - Asymmetric search (compressed vs full query) - Slower than HNSW but very memory-efficient - Lower recall due to quantization error - Best for: Memory-constrained environments</p> <p>Scaling: 10x more vectors = 2x build time, 12x slower queries, 6x more memory</p> <p>Configuration (used in benchmarks):</p> <pre><code>M:     16  // Subquantizers (16 \u00d7 8 = 128 dims)\nNbits: 8   // 8 bits = 256 centroids per subquantizer\n</code></pre> <p>Compression Ratio:  - Original: 128 dims \u00d7 4 bytes = 512 bytes/vector - Compressed: 16 subquantizers \u00d7 1 byte = 16 bytes/vector - Ratio: 32x</p>"},{"location":"benchmarks/#ivfpq-index-combined","title":"IVFPQ Index (Combined)","text":"<pre><code>Dataset    | Build   | Avg Query | QPS | Memory  | Recall\n10K        | 2,313ms | 0.44ms    | 2,297| 0.25 MB | 8.8%\n100K       | 8,506ms | 1.94ms    | 516 | 1.04 MB | 4.0%\n</code></pre> <p>Characteristics: - Best memory efficiency (combines IVF clustering + PQ compression) - Moderate query speed - Lower recall than IVF or PQ alone (compounded approximations) - Best for: Large-scale, memory-constrained deployments</p> <p>Scaling: 10x more vectors = 3.7x build time, 4.4x slower queries, 4x more memory</p> <p>Configuration (used in benchmarks):</p> <pre><code>nlist:  316 (100K), 100 (10K)\nM:      8                         // Fewer subquantizers\nNbits:  8\nnprobe: 10\n</code></pre> <p>Low Recall Issue: The 4-9% recall is concerning and likely due to: 1. Small nprobe (10 clusters) 2. Aggressive compression (M=8) 3. Need for more training data</p> <p>Production Settings (for 90%+ recall):</p> <pre><code>config := ivfpq.Config{\n    Nlist:  316,\n    M:      16,    // More subquantizers\n    Nbits:  8,\n    Nprobe: 50,    // Search more clusters\n}\n// Train with at least 50K vectors\nidx.Train(vectors[:50000])\n</code></pre>"},{"location":"benchmarks/#performance-trade-offs","title":"Performance Trade-offs","text":""},{"location":"benchmarks/#speed-vs-accuracy","title":"Speed vs Accuracy","text":"<pre><code>Index    | QPS (100K) | Recall | Use Case\n---------|------------|--------|----------------------------------\nHNSW     | 24,087     | 99.96% | Production queries, speed critical\nIVF      | 343        | 26%*   | Adjustable speed/accuracy balance\nIVFPQ    | 516        | 4%*    | Memory constrained, tunable\nPQ       | 50         | 19.7%  | Extreme compression needed\nFlat     | 36         | 100%   | Ground truth, small datasets\n\n*With nprobe=10. Increase for better recall.\n</code></pre>"},{"location":"benchmarks/#memory-vs-accuracy","title":"Memory vs Accuracy","text":"<pre><code>Index    | Memory (100K) | Compression | Recall | Notes\n---------|---------------|-------------|--------|------------------\nFlat     | ~48.8 MB      | 1x          | 100%   | Baseline\nHNSW     | 97.66 MB      | 0.5x        | 99.96% | More memory used\nIVF      | 48.98 MB      | 1x          | 26%*   | Similar to Flat\nPQ       | 1.65 MB       | 29.6x       | 19.7%  | High compression\nIVFPQ    | 1.04 MB       | 46.9x       | 4%*    | Highest compression\n\n*Recall improves with parameter tuning\n</code></pre>"},{"location":"benchmarks/#build-time-vs-query-speed","title":"Build Time vs Query Speed","text":"<pre><code>Index    | Build (100K) | Avg Query | Build/Query Tradeoff\n---------|--------------|-----------|---------------------\nFlat     | 2ms          | 27.89ms   | No build cost\nHNSW     | 18,061ms     | 0.04ms    | High build, fast query\nIVF      | 4,572ms      | 2.92ms    | Moderate both\nPQ       | 4,034ms      | 19.86ms   | Moderate build, slow query\nIVFPQ    | 8,506ms      | 1.94ms    | High build, moderate query\n</code></pre>"},{"location":"benchmarks/#scaling-analysis","title":"Scaling Analysis","text":""},{"location":"benchmarks/#query-time-scaling","title":"Query Time Scaling","text":"<pre><code>Index    | 10K Query | 100K Query | Ratio | Complexity\n---------|-----------|------------|-------|------------\nFlat     | 2.58ms    | 27.89ms    | 10.8x | O(n)\nHNSW     | 0.06ms    | 0.04ms     | 0.67x | O(log n)\nIVF      | 0.34ms    | 2.92ms     | 8.6x  | O(\u221an)\nPQ       | 1.66ms    | 19.86ms    | 12x   | O(n)\nIVFPQ    | 0.44ms    | 1.94ms     | 4.4x  | O(\u221an)\n</code></pre> <p>Key Insight: HNSW actually gets faster with more data (better graph connectivity). IVF scales well. PQ/Flat scale linearly.</p>"},{"location":"benchmarks/#memory-scaling","title":"Memory Scaling","text":"<pre><code>Index    | 10K Memory | 100K Memory | Ratio\n---------|------------|-------------|-------\nHNSW     | 9.77 MB    | 97.66 MB    | 10x\nIVF      | 4.93 MB    | 48.98 MB    | 9.9x\nPQ       | 0.28 MB    | 1.65 MB     | 5.9x\nIVFPQ    | 0.25 MB    | 1.04 MB     | 4.2x\n</code></pre> <p>Key Insight: Quantized indexes (PQ, IVFPQ) have better-than-linear memory scaling due to shared codebooks.</p>"},{"location":"benchmarks/#real-world-scenarios","title":"Real-World Scenarios","text":""},{"location":"benchmarks/#scenario-1-e-commerce-product-search","title":"Scenario 1: E-commerce Product Search","text":"<p>Requirements: 1M products, 512-dim embeddings, &lt;100ms latency, 95%+ recall</p> <p>Recommended: HNSW with dimension reduction</p> <pre><code>// Reduce to 256 dims with PCA first\nidx, _ := hnsw.New(256, \"cosine\", hnsw.Config{\n    M:              32,\n    EfConstruction: 200,\n    EfSearch:       100,\n})\n</code></pre> <p>Expected Performance: - Query latency: ~0.5ms (well under 100ms) - Memory: ~2GB - Recall: 98%+</p>"},{"location":"benchmarks/#scenario-2-document-similarity-rag-system","title":"Scenario 2: Document Similarity (RAG System)","text":"<p>Requirements: 100K documents, 1536-dim (OpenAI embeddings), memory &lt;1GB, 90%+ recall</p> <p>Recommended: IVFPQ with careful tuning</p> <pre><code>config := ivfpq.Config{\n    Nlist:  316,\n    M:      32,      // 1536/32 = 48 dims per subquantizer\n    Nbits:  8,\n    Nprobe: 50,\n}\nidx.Train(vectors[:20000])  // 20% for training\n</code></pre> <p>Expected Performance: - Memory: ~400MB - Query latency: ~5ms - Recall: 92%+</p>"},{"location":"benchmarks/#scenario-3-image-deduplication","title":"Scenario 3: Image Deduplication","text":"<p>Requirements: 10M images, 128-dim perceptual hashes, exact duplicates only</p> <p>Recommended: Start with IVF, fall back to Flat for exact matches</p> <pre><code>// Quick filter with IVF\nivfIdx, _ := ivf.New(128, \"l2\", ivf.DefaultConfig(10000000))\ncandidates, _ := ivfIdx.Search(query, 100, 20)\n\n// Exact verification\nflatIdx, _ := flat.New(128, \"l2\")\nflatIdx.Add(candidateVectors)\nexactMatches, _ := flatIdx.Search(query, 10)\n</code></pre>"},{"location":"benchmarks/#scenario-4-real-time-recommendation","title":"Scenario 4: Real-time Recommendation","text":"<p>Requirements: 500K users, 64-dim, &lt;10ms latency, update frequently</p> <p>Recommended: HNSW with incremental updates</p> <pre><code>idx, _ := hnsw.New(64, \"cosine\", hnsw.Config{\n    M:              16,\n    EfConstruction: 100,  // Lower for faster updates\n    EfSearch:       50,\n})\n\n// Incremental updates\ngo func() {\n    for newUser := range userUpdates {\n        idx.Add([]vector.Vector{newUser})\n    }\n}()\n</code></pre> <p>Expected Performance: - Query latency: ~0.1ms - Update latency: ~5ms - Memory: ~100MB</p>"},{"location":"benchmarks/#tuning-guidelines","title":"Tuning Guidelines","text":""},{"location":"benchmarks/#hnsw-tuning","title":"HNSW Tuning","text":"<p>For Higher Recall:</p> <pre><code>config := hnsw.Config{\n    M:              32,    // 16 \u2192 32\n    EfConstruction: 400,   // 200 \u2192 400\n    EfSearch:       200,   // 50 \u2192 200\n}\n</code></pre> <ul> <li>Recall: 99.9% \u2192 99.99%</li> <li>Memory: +50%</li> <li>Build time: +100%</li> <li>Query time: +100%</li> </ul> <p>For Faster Queries:</p> <pre><code>config := hnsw.Config{\n    M:              16,\n    EfConstruction: 200,\n    EfSearch:       20,    // 50 \u2192 20\n}\n</code></pre> <ul> <li>Recall: 99.96% \u2192 98%</li> <li>Query time: 0.04ms \u2192 0.01ms</li> </ul> <p>For Faster Builds:</p> <pre><code>config := hnsw.Config{\n    M:              16,\n    EfConstruction: 100,   // 200 \u2192 100\n    EfSearch:       50,\n}\n</code></pre> <ul> <li>Build time: -50%</li> <li>Recall: 99.96% \u2192 99%</li> </ul>"},{"location":"benchmarks/#ivfivfpq-tuning","title":"IVF/IVFPQ Tuning","text":"<p>For Higher Recall:</p> <pre><code>// At search time\nresults, _ := idx.Search(query, k, 100)  // nprobe: 10 \u2192 100\n</code></pre> <ul> <li>Recall: 26% \u2192 90%+</li> <li>Query time: +10x</li> </ul> <p>For Faster Queries:</p> <pre><code>// Increase cluster count\nconfig.Nlist = 1000  // 316 \u2192 1000\n</code></pre> <ul> <li>Query time: -50%</li> <li>Recall: -10%</li> <li>Build time: +50%</li> </ul> <p>For Less Memory:</p> <pre><code>// Reduce subquantizers (IVFPQ)\nconfig.M = 4  // 8 \u2192 4 (for 128-dim vectors)\n</code></pre> <ul> <li>Memory: -50%</li> <li>Recall: -20%</li> </ul>"},{"location":"benchmarks/#benchmark-reproducibility","title":"Benchmark Reproducibility","text":""},{"location":"benchmarks/#running-benchmarks-yourself","title":"Running Benchmarks Yourself","text":"<pre><code># Clone repository\ngit clone https://github.com/tahcohcat/gofaiss\ncd gofaiss/benchmark\n\n# Quick benchmark\nmake benchmark-quick\n\n# Full statistical analysis (5 runs)\nmake benchmark-full\n\n# Generate visualizations\nmake visualize\n</code></pre>"},{"location":"benchmarks/#custom-benchmark-configuration","title":"Custom Benchmark Configuration","text":"<p>Edit <code>benchmark_comparison.go</code>:</p> <pre><code>configs := []BenchmarkConfig{\n    {\n        Dimensions:  128,\n        NumVectors:  50000,    // Your dataset size\n        NumQueries:  500,      // Number of queries\n        K:           10,       // k for k-NN\n        Seed:        42,       // For reproducibility\n        OutputFile:  \"custom_results.json\",\n    },\n}\n</code></pre>"},{"location":"benchmarks/#benchmark-environment","title":"Benchmark Environment","text":"<p>For consistent results:</p> <pre><code># Set CPU to performance mode\nsudo cpupower frequency-set -g performance\n\n# Pin to specific cores\ntaskset -c 0-7 ./benchmark_comparison\n\n# Close background apps\n# Disable CPU frequency scaling\n# Use same hardware for comparisons\n</code></pre>"},{"location":"benchmarks/#statistical-analysis","title":"Statistical Analysis","text":"<p>Benchmarks should be run multiple times for statistical validity:</p> <pre><code># Run 5 times\nmake benchmark-full\n\n# Results include:\n# - Mean performance\n# - Standard deviation\n# - 95% confidence intervals\n# - Coefficient of variation (CV)\n</code></pre> <p>Good benchmark reproducibility: CV &lt; 5% Acceptable reproducibility: CV &lt; 10% High variance: CV &gt; 10% (investigate environmental factors)</p>"},{"location":"benchmarks/#comparison-with-other-libraries","title":"Comparison with Other Libraries","text":""},{"location":"benchmarks/#vs-python-faiss","title":"vs Python FAISS","text":"<p>GoFAISS Advantages: - No Python runtime required - Native Go integration - Smaller binary size (~10MB vs 100MB+) - Easier deployment (single binary)</p> <p>Python FAISS Advantages: - More index types (GPU, LSH, IMI) - More mature and tested - GPU acceleration - Larger community</p> <p>Performance: Comparable for supported index types (within 10-20%)</p>"},{"location":"benchmarks/#vs-hnswlib-go","title":"vs hnswlib-go","text":"<p>GoFAISS Advantages: - Multiple index types (not just HNSW) - Product quantization support - More comprehensive API - Better documentation</p> <p>Performance Comparison (100K vectors, 128 dims):</p> <pre><code>Library       | QPS    | Recall | Memory\nGoFAISS HNSW  | 24,087 | 99.96% | 97.7 MB\nhnswlib-go    | ~19,000| 98.5%  | ~100 MB\n</code></pre> <p>GoFAISS is ~25% faster in our benchmarks with similar recall and memory usage.</p>"},{"location":"benchmarks/#historical-performance","title":"Historical Performance","text":""},{"location":"benchmarks/#version-history","title":"Version History","text":"<p>v0.1.0 (Current): - HNSW: 24K QPS @ 99.96% recall - Build optimizations: 20% faster than v0.0.1 - Memory optimizations: 15% reduction</p> <p>v0.0.1 (Initial): - HNSW: 20K QPS @ 99% recall - Basic implementations</p>"},{"location":"benchmarks/#future-optimizations","title":"Future Optimizations","text":"<p>Planned improvements:</p> <ol> <li>SIMD Optimizations: 2-4x distance calculation speedup</li> <li>Memory-Mapped Storage: Support 10M+ vectors on limited RAM</li> <li>GPU Acceleration: Experimental GPU support for HNSW</li> <li>Distributed Search: Multi-node search for 100M+ vectors</li> <li>Compressed Graphs: Reduce HNSW memory by 30%</li> </ol>"},{"location":"benchmarks/#contributing-benchmarks","title":"Contributing Benchmarks","text":"<p>We welcome benchmark contributions:</p> <ol> <li>Run benchmarks on your hardware</li> <li>Submit results with system specs</li> <li>Include dataset characteristics</li> <li>Note any special configurations</li> </ol> <p>Format:</p> <pre><code>{\n  \"hardware\": \"AMD Ryzen 9 5950X, 32GB DDR4\",\n  \"os\": \"Ubuntu 22.04\",\n  \"go_version\": \"1.21.5\",\n  \"results\": [...]\n}\n</code></pre>"},{"location":"benchmarks/#conclusion","title":"Conclusion","text":"<p>Key Takeaways:</p> <ol> <li>HNSW is the best choice for most production workloads (24K QPS, 99.96% recall)</li> <li>IVFPQ provides the best memory efficiency (46x compression)</li> <li>IVF offers controllable speed/accuracy tradeoff with nprobe tuning</li> <li>Flat is essential for ground truth and small datasets (&lt;10K)</li> <li>Scaling is excellent: HNSW queries don't slow down with more data</li> </ol> <p>Quick Decision Matrix:</p> <ul> <li>Need speed? \u2192 HNSW</li> <li>Need memory efficiency? \u2192 IVFPQ</li> <li>Need exact results? \u2192 Flat</li> <li>Need balance? \u2192 IVF with tuned nprobe</li> <li>Need compression? \u2192 PQ</li> </ul> <p>For detailed usage examples, see the Getting Started guide.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you get started with GoFAISS, a pure Go implementation of Facebook's FAISS library for efficient similarity search and clustering of dense vectors.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>Install GoFAISS using <code>go get</code>:</p> <pre><code>go get github.com/tahcohcat/gofaiss\n</code></pre>"},{"location":"getting-started/#core-concepts","title":"Core Concepts","text":""},{"location":"getting-started/#vectors","title":"Vectors","text":"<p>GoFAISS works with dense vectors represented as <code>[]float32</code>. Each vector can have an associated ID and is stored in the <code>vector.Vector</code> struct:</p> <pre><code>type Vector struct {\n    ID   int64\n    Data []float32\n    Norm float32 // Precomputed norm for cosine similarity\n}\n</code></pre>"},{"location":"getting-started/#distance-metrics","title":"Distance Metrics","text":"<p>GoFAISS supports three distance metrics:</p> <ul> <li>L2 (Euclidean): <code>\"l2\"</code> - Standard Euclidean distance</li> <li>Cosine Similarity: <code>\"cosine\"</code> - Measures angular distance</li> <li>Inner Product: <code>\"dot\"</code> - Dot product similarity</li> </ul>"},{"location":"getting-started/#index-types","title":"Index Types","text":"<p>GoFAISS provides several index types optimized for different use cases:</p> Index Type Speed Memory Accuracy Use Case Flat Slow High 100% Exact search, small datasets HNSW Fast High ~98% Fast approximate search IVF Medium Medium ~95% Balanced speed/memory PQ Medium Low ~90% Memory-constrained environments IVFPQ Fast Low ~93% Large-scale with compression"},{"location":"getting-started/#quick-start-examples","title":"Quick Start Examples","text":""},{"location":"getting-started/#1-flat-index-exact-search","title":"1. Flat Index (Exact Search)","text":"<p>Best for small datasets where you need 100% accuracy:</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"log\"\n\n    \"github.com/tahcohcat/gofaiss/pkg/index/flat\"\n    \"github.com/tahcohcat/gofaiss/pkg/vector\"\n)\n\nfunc main() {\n    // Create index with 128-dimensional vectors using L2 distance\n    dim := 128\n    idx, err := flat.New(dim, \"l2\")\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    // Generate random vectors for testing\n    vectors := vector.GenerateRandom(1000, dim, 42)\n\n    // Add vectors to index\n    if err := idx.Add(vectors); err != nil {\n        log.Fatal(err)\n    }\n\n    // Search for 10 nearest neighbors\n    query := make([]float32, dim)\n    results, err := idx.Search(query, 10)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    // Print results\n    for i, r := range results {\n        fmt.Printf(\"%d: ID=%d, Distance=%.4f\\n\", i+1, r.ID, r.Distance)\n    }\n}\n</code></pre>"},{"location":"getting-started/#2-hnsw-index-fast-approximate-search","title":"2. HNSW Index (Fast Approximate Search)","text":"<p>Best for fast similarity search with high accuracy:</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"log\"\n\n    \"github.com/tahcohcat/gofaiss/pkg/index/hnsw\"\n    \"github.com/tahcohcat/gofaiss/pkg/vector\"\n)\n\nfunc main() {\n    dim := 128\n\n    // Configure HNSW parameters\n    config := hnsw.Config{\n        M:              16,  // Number of connections per layer\n        EfConstruction: 200, // Quality during build (higher = better)\n        EfSearch:       50,  // Quality during search (higher = better)\n    }\n\n    // Create HNSW index\n    idx, err := hnsw.New(dim, \"l2\", config)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    // Add vectors\n    vectors := vector.GenerateRandom(10000, dim, 42)\n    if err := idx.Add(vectors); err != nil {\n        log.Fatal(err)\n    }\n\n    // Adjust search quality at runtime\n    idx.SetEfSearch(100) // Higher = better accuracy, slower\n\n    // Search\n    query := vectors[0].Data\n    results, err := idx.Search(query, 10)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    fmt.Printf(\"Found %d neighbors\\n\", len(results))\n}\n</code></pre>"},{"location":"getting-started/#3-product-quantization-memory-efficient","title":"3. Product Quantization (Memory Efficient)","text":"<p>Best for large datasets with memory constraints:</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"log\"\n\n    \"github.com/tahcohcat/gofaiss/pkg/index/pq\"\n    \"github.com/tahcohcat/gofaiss/pkg/vector\"\n)\n\nfunc main() {\n    dim := 128\n\n    // Configure PQ\n    config := pq.Config{\n        M:     16, // Number of subquantizers (must divide dim evenly)\n        Nbits: 8,  // Bits per code (8 = 256 centroids)\n    }\n\n    idx, err := pq.NewIndex(dim, config)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    // PQ requires training\n    trainingVectors := vector.GenerateRandom(5000, dim, 42)\n    if err := idx.Train(trainingVectors); err != nil {\n        log.Fatal(err)\n    }\n\n    // Add vectors (automatically compressed)\n    vectors := vector.GenerateRandom(100000, dim, 43)\n    if err := idx.Add(vectors); err != nil {\n        log.Fatal(err)\n    }\n\n    // Search\n    query := vectors[0].Data\n    results, err := idx.Search(query, 10)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    // Check compression stats\n    stats := idx.Stats()\n    fmt.Printf(\"Memory: %.2f MB\\n\", stats.MemoryUsageMB)\n    fmt.Printf(\"Compression: ~%.1fx\\n\",\n        float64(len(vectors)*dim*4)/(stats.MemoryUsageMB*1024*1024))\n}\n</code></pre>"},{"location":"getting-started/#4-ivf-index-inverted-file","title":"4. IVF Index (Inverted File)","text":"<p>Best for balanced speed and memory usage:</p> <pre><code>package main\n\nimport (\n    \"log\"\n\n    \"github.com/tahcohcat/gofaiss/pkg/index/ivf\"\n    \"github.com/tahcohcat/gofaiss/pkg/vector\"\n)\n\nfunc main() {\n    dim := 128\n    numVectors := 100000\n\n    // Configure IVF\n    config := ivf.Config{\n        Metric: \"l2\",\n        Nlist:  1000, // Number of clusters (typically sqrt(N))\n    }\n\n    idx, err := ivf.New(dim, \"l2\", config)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    // Train clustering\n    trainingVectors := vector.GenerateRandom(10000, dim, 42)\n    if err := idx.Train(trainingVectors); err != nil {\n        log.Fatal(err)\n    }\n\n    // Add vectors\n    vectors := vector.GenerateRandom(numVectors, dim, 43)\n    if err := idx.Add(vectors); err != nil {\n        log.Fatal(err)\n    }\n\n    // Set search parameters\n    idx.SetNProbe(10) // Number of clusters to search (higher = better)\n\n    // Search\n    query := vectors[0].Data\n    results, _ := idx.Search(query, 10)\n    log.Printf(\"Found %d results\\n\", len(results))\n}\n</code></pre>"},{"location":"getting-started/#batch-operations","title":"Batch Operations","text":"<p>All indexes support batch search for better performance:</p> <pre><code>// Prepare multiple queries\nqueries := make([][]float32, 100)\nfor i := 0; i &lt; 100; i++ {\n    queries[i] = make([]float32, dim)\n}\n\n// Batch search\nresults, err := idx.BatchSearch(queries, 10)\nif err != nil {\n    log.Fatal(err)\n}\n\n// results is [][]vector.SearchResult\nfor i, queryResults := range results {\n    fmt.Printf(\"Query %d: found %d neighbors\\n\", i, len(queryResults))\n}\n</code></pre>"},{"location":"getting-started/#persistence","title":"Persistence","text":"<p>Save and load indexes to disk:</p> <pre><code>import \"github.com/tahcohcat/gofaiss/pkg/storage\"\n\n// Save with gzip compression\nerr := storage.SaveIndex(idx, \"index.faiss.gz\", true)\nif err != nil {\n    log.Fatal(err)\n}\n\n// Load from disk\nloadedIdx := &amp;flat.Index{} // Use appropriate type\nerr = storage.LoadIndex(loadedIdx, \"index.faiss.gz\", true)\nif err != nil {\n    log.Fatal(err)\n}\n</code></pre>"},{"location":"getting-started/#index-statistics","title":"Index Statistics","text":"<p>Get information about your index:</p> <pre><code>stats := idx.Stats()\nfmt.Printf(\"Vectors: %d\\n\", stats.TotalVectors)\nfmt.Printf(\"Dimension: %d\\n\", stats.Dimension)\nfmt.Printf(\"Memory: %.2f MB\\n\", stats.MemoryUsageMB)\nfmt.Printf(\"Index Type: %s\\n\", stats.IndexType)\n\n// HNSW-specific info\nif info, ok := stats.ExtraInfo[\"maxLevel\"]; ok {\n    fmt.Printf(\"Max Level: %v\\n\", info)\n}\n</code></pre>"},{"location":"getting-started/#performance-tips","title":"Performance Tips","text":""},{"location":"getting-started/#choosing-an-index-type","title":"Choosing an Index Type","text":"<ol> <li>&lt; 10K vectors: Use <code>Flat</code> for exact results</li> <li>10K - 1M vectors: Use <code>HNSW</code> for speed or <code>IVF</code> for balance</li> <li>&gt; 1M vectors: Use <code>IVFPQ</code> for memory efficiency</li> <li>Need exact search: Use <code>Flat</code> regardless of size</li> </ol>"},{"location":"getting-started/#hnsw-tuning","title":"HNSW Tuning","text":"<ul> <li>M: Higher = better recall, more memory (typical: 16-64)</li> <li>efConstruction: Higher = better quality, slower build (typical: 100-500)</li> <li>efSearch: Adjust at runtime for speed/accuracy tradeoff</li> </ul>"},{"location":"getting-started/#ivf-tuning","title":"IVF Tuning","text":"<ul> <li>nlist: Typically <code>sqrt(num_vectors)</code>, range: [100, 65536]</li> <li>nprobe: Higher = better recall, slower search (typical: 1-20)</li> </ul>"},{"location":"getting-started/#pq-tuning","title":"PQ Tuning","text":"<ul> <li>M: Higher = better accuracy, more computation (typical: 8-64)</li> <li>nbits: Higher = better accuracy, more memory (typical: 8)</li> </ul>"},{"location":"getting-started/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/#cosine-similarity-search","title":"Cosine Similarity Search","text":"<pre><code>idx, err := hnsw.New(dim, \"cosine\", hnsw.DefaultConfig())\n</code></pre>"},{"location":"getting-started/#custom-vector-ids","title":"Custom Vector IDs","text":"<pre><code>vectors := []vector.Vector{\n    {ID: 1001, Data: vec1},\n    {ID: 1002, Data: vec2},\n}\nidx.Add(vectors)\n</code></pre>"},{"location":"getting-started/#removing-vectors-hnsw-only","title":"Removing Vectors (HNSW only)","text":"<pre><code>err := idx.Remove(vectorID)\n</code></pre>"},{"location":"getting-started/#generate-test-data","title":"Generate Test Data","text":"<pre><code>// Random vectors with fixed seed for reproducibility\nvectors := vector.GenerateRandom(1000, 128, 42)\n\n// Normalize for cosine similarity\nfor i := range vectors {\n    vector.NormalizeInPlace(vectors[i].Data)\n}\n</code></pre>"},{"location":"getting-started/#error-handling","title":"Error Handling","text":"<p>GoFAISS returns errors for:</p> <ul> <li>Dimension mismatches</li> <li>Untrained indexes (IVF, PQ, IVFPQ)</li> <li>Invalid configurations</li> <li>File I/O errors</li> </ul> <p>Always check errors:</p> <pre><code>if err := idx.Add(vectors); err != nil {\n    log.Fatalf(\"Failed to add vectors: %v\", err)\n}\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Read the API Reference for detailed documentation</li> <li>Check the Architecture guide to understand internals</li> <li>See Benchmarks for performance comparisons</li> <li>Review the examples directory</li> </ul>"},{"location":"getting-started/#complete-example","title":"Complete Example","text":"<p>Here's a complete working example combining multiple concepts:</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"log\"\n    \"time\"\n\n    \"github.com/tahcohcat/gofaiss/pkg/index/hnsw\"\n    \"github.com/tahcohcat/gofaiss/pkg/storage\"\n    \"github.com/tahcohcat/gofaiss/pkg/vector\"\n)\n\nfunc main() {\n    dim := 128\n    numVectors := 50000\n\n    // Create index\n    config := hnsw.DefaultConfig()\n    idx, err := hnsw.New(dim, \"l2\", config)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    // Generate and add vectors\n    fmt.Println(\"Generating vectors...\")\n    vectors := vector.GenerateRandom(numVectors, dim, 42)\n\n    fmt.Println(\"Building index...\")\n    start := time.Now()\n    if err := idx.Add(vectors); err != nil {\n        log.Fatal(err)\n    }\n    buildTime := time.Since(start)\n\n    // Benchmark search\n    fmt.Println(\"Benchmarking search...\")\n    queries := vector.GenerateRandom(100, dim, 43)\n\n    start = time.Now()\n    for _, q := range queries {\n        _, err := idx.Search(q.Data, 10)\n        if err != nil {\n            log.Fatal(err)\n        }\n    }\n    searchTime := time.Since(start)\n\n    // Print stats\n    stats := idx.Stats()\n    fmt.Printf(\"\\n=== Results ===\\n\")\n    fmt.Printf(\"Vectors: %d\\n\", stats.TotalVectors)\n    fmt.Printf(\"Build time: %v\\n\", buildTime)\n    fmt.Printf(\"Search time: %v (%.2f ms/query)\\n\",\n        searchTime, float64(searchTime.Milliseconds())/100.0)\n    fmt.Printf(\"Memory: %.2f MB\\n\", stats.MemoryUsageMB)\n\n    // Save index\n    fmt.Println(\"\\nSaving index...\")\n    if err := storage.SaveIndex(idx, \"my_index.faiss.gz\", true); err != nil {\n        log.Fatal(err)\n    }\n\n    fmt.Println(\"Done!\")\n}\n</code></pre>"},{"location":"roadmap/","title":"gofaiss Roadmap","text":"<p>This roadmap outlines the planned development path for gofaiss, from its current state toward a mature, production-ready Go-native vector search library. The following are just ideas on what could be done in a loose logical progression. It is by no means set  in stone or guaranteed. </p>"},{"location":"roadmap/#phase-0-foundation-current-immediate","title":"Phase 0: Foundation (current / immediate)","text":"<p>Goals: - Solidify core index and metric implementations - Provide basic persistence and benchmark scaffolding - Launch initial version (v0.1)</p> <p>Planned Deliverables: - Flat (exact) index - IVF, IVFPQ, HNSW index types - Metrics: L2, Cosine, Inner Product - Serialization / Save &amp; Load (gob, JSON, gzip) - Benchmark suite (simple datasets) - Basic CLI tool - README, docs, and project structure - Tag v0.1 release  </p>"},{"location":"roadmap/#phase-1-developer-experience-stability-v02-v03","title":"Phase 1: Developer Experience &amp; Stability (v0.2 \u2192 v0.3)","text":"<p>Goals: - Make gofaiss easier to adopt, test, and contribute to - Harden the library for everyday use</p> <p>Key Items: - GitHub Actions CI / nightly builds - More benchmark comparisons vs popular alternatives (e.g. hnswlib-go, faiss-cgo) - Example applications / sample datasets - Interface stability &amp; GoDoc enhancements - Better error handling and validation - Benchmarks with more realistic datasets (e.g. SIFT1M, GloVe) - \u201cGood first issue\u201d tags for new contributors - Release v0.2  </p>"},{"location":"roadmap/#phase-2-performance-scaling-concurrency-v03-v04","title":"Phase 2: Performance, Scaling &amp; Concurrency (v0.3 \u2192 v0.4)","text":"<p>Goals: - Enable larger datasets, faster queries, concurrent use - Reduce memory overheads, support persistence for large indexes</p> <p>Features &amp; Enhancements: - Concurrency-safe <code>Search</code> / <code>Add</code> operations - Sharded or segmented \u201cFlat\u201d and \u201cIVF\u201d partitions - Memory-mapped backing (mmap) for large indexes - Batch query support - Hybrid index combinations (e.g. IVF + HNSW) - Parallelized search / probe for IVF - SIMD / vectorized distance computation optimizations - Incremental updates / deletions to index - Release v0.3  </p>"},{"location":"roadmap/#phase-3-serving-ecosystem-integration-v04-v05","title":"Phase 3: Serving, Ecosystem &amp; Integration (v0.4 \u2192 v0.5)","text":"<p>Goals: - Turn gofaiss into a building block in larger systems - Interoperate with other tools, serve remote clients</p> <p>Features &amp; Integrations: - HTTP / gRPC API for query / index operations - Docker + Helm charts - Integration adapters (e.g. embedding pipelines, vector DBs) - Example embedding ingestion pipelines (e.g. CLIP, ONNX) - Monitoring, metrics endpoints (latency, memory usage) - Snapshotting, versioning, merging indexes - Release v0.4  </p>"},{"location":"roadmap/#phase-4-maturity-community-v05-v10","title":"Phase 4: Maturity &amp; Community (v0.5 \u2192 v1.0)","text":"<p>Goals: - Establish stable APIs, expand ecosystem, attract users &amp; contributors - Ensure production robustness</p> <p>Key Milestones: - API stability / semantic versioning - Extended metric support (Manhattan, Hamming, Jaccard etc.) - Support for very large scale (10M+ vectors) - Distributed indexing / sharding across nodes - Community benchmark suite (contributors can add datasets) - Governance / contribution guidelines - Release v1.0  </p>"},{"location":"roadmap/#ideas-stretch-goals","title":"Ideas &amp; Stretch Goals","text":"<ul> <li>Disk-resident graph caching (for HNSW)  </li> <li>Hybrid quantization + indexing methods  </li> <li>Support GPU acceleration (via external bindings)  </li> <li>WebAssembly build target  </li> <li>Client libraries (Python / JS) for remote access  </li> <li>Auto-tuning of index parameters for a given dataset  </li> </ul>"},{"location":"roadmap/#how-you-can-help","title":"How You Can Help","text":"<ul> <li>Try using gofaiss and report issues or performance comparisons  </li> <li>Submit benchmark results or dataset support  </li> <li>Implement new metric / index types  </li> <li>Help writing docs, tutorials, examples  </li> <li>Review PRs and propose enhancements  </li> </ul>"}]}